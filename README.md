# AI対話振り返りメディテーションシステム

**Version:** 2.3 (MVPStreamlit版)  
**Last Updated:** 2025/01/XX

「言葉にできないモヤモヤ」を可視化し、就寝前のメンタルヘルスを整えるAIコーチングシステム。

---

## 📚 目次

1. [概要](#概要)
2. [機能](#機能)
3. [ドキュメント](#ドキュメント)
4. [システムアーキテクチャ](#システムアーキテクチャ)
5. [機能要件](#機能要件)
6. [実装要件](#実装要件)
7. [プロジェクト構成](#プロジェクト構成)
8. [技術スタック](#技術スタック)
9. [開発チーム](#開発チーム)
10. [開発ロードマップ](#開発ロードマップ)
11. [注意事項](#注意事項)

---

## 概要

ユーザーの「言葉（テキスト）」だけでなく、「表情（映像）」や「声のトーン（音声）」という非言語情報をマルチモーダルに解析し、ユーザー自身も気づいていない感情の機微を指摘・受容する対話体験を提供します。

### ターゲット体験

- **手軽さ**: ユーザーは就寝前、カメラに向かってその日の出来事を「話す」だけ（タイピング不要）
- **気づき**: 「辛いと言葉では言っているが、少し笑っている」などの矛盾や隠れた感情をAIがフィードバックする
- **記録**: 感情の推移が自動でログ化され、客観的に自分を見つめ直せる

### プロジェクト方針

- **UX優先**: ユーザーの自然な会話を阻害しないよう、リアルタイム映像処理 (`streamlit-webrtc`) と音声入力 (`st.audio_input`) を採用する
- **リソース配慮**: サーバー負荷を考慮し、映像解析は間引き処理を行い、音声解析は軽量な手法を用いる
- **堅牢性**: エラーハンドリングとリソース管理（ファイル削除等）を徹底する

---

## 機能

- 🎭 **感情入力**: 2次元感情プロット（快/不快 × 覚醒/落ち着き）による感情の可視化入力
- 📹 **表情認識**: 録画動画からGPT-4o Vision APIを使用した表情分析
- 🎤 **音声認識**: Whisper APIによる音声の文字起こし
- 🤖 **AI対話**: GPT-4o-miniによる共感的なカウンセリング対話
- 📊 **対話履歴**: 過去の対話履歴の表示と管理
- 💾 **データ保存**: Supabase (PostgreSQL)による対話履歴の永続化
- 👤 **ユーザー管理**: ユーザー名による対話履歴の分離管理

---

## 📖 ドキュメント

### セットアップ・使用方法

**📚 [MANUAL.md](MANUAL.md)** に詳細な手順を記載しています：

- 🚀 **クイックスタート**: 5分で始められる最短セットアップ手順
- 📝 **環境構築**: 仮想環境の作成からパッケージインストールまで
- 🎯 **使用方法**: アプリの基本的な使い方
- 🔧 **トラブルシューティング**: よくある問題と解決方法

**初心者の方も経験者の方も、まず [MANUAL.md](MANUAL.md) をご覧ください！**

---

## システムアーキテクチャ

### 実行環境

**MVP段階では、Streamlit上のみで動作することを前提とする。**

- **実行環境**: Streamlit Cloud または ローカル環境
- **言語**: Python 3.10+
- **デプロイ**: Streamlitアプリケーションとして単一実行

> **注意**: 本仕様書はMVP版のため、ConoHa VPSなどの外部サーバーは不要。将来的なスケールアップ時に検討する。

### システムフロー図

```
┌─────────────────────────────────────────────────────────────┐
│  ステップ1: 感情入力                                           │
│  ┌──────────────────────────────────────┐                   │
│  │  2D感情プロット (Plotly)              │                   │
│  │  X軸: 不快 ← 0 → 快                  │                   │
│  │  Y軸: 非覚醒 ← 0 → 覚醒              │                   │
│  └──────────────────────────────────────┘                   │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│  ステップ2: 録画録音                                           │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐              │
│  │録画開始  │ → │録画中... │ → │録画停止  │              │
│  │(WebRTC)  │    │          │    │          │              │
│  └──────────┘    └──────────┘    └──────────┘              │
│                        ↓                                     │
│              ┌─────────────────┐                            │
│              │ WebM動画データ   │                            │
│              └─────────────────┘                            │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│  並列処理                                                      │
│  ┌──────────────┐         ┌──────────────┐                 │
│  │ 表情認識      │         │ 文字起こし    │                 │
│  │              │         │              │                 │
│  │ OpenCV       │         │ Whisper API  │                 │
│  │ フレーム抽出  │         │              │                 │
│  │ (5秒間隔)    │         │              │                 │
│  │              │         │              │                 │
│  │ GPT-4o       │         │              │                 │
│  │ Vision API   │         │              │                 │
│  └──────┬───────┘         └──────┬───────┘                 │
│         │                        │                          │
│         └──────────┬─────────────┘                          │
│                    ↓                                        │
│         ┌──────────────────────┐                           │
│         │ 統合プロンプト構築     │                           │
│         │ (感情座標 + 表情 + 文字起こし) │                   │
│         └──────────┬───────────┘                           │
│                    ↓                                        │
│         ┌──────────────────────┐                           │
│         │   GPT-4o-mini API    │                           │
│         └──────────┬───────────┘                           │
│                    ↓                                        │
│  ┌─────────────────────────────────┐                      │
│  │ AI応答生成                        │                      │
│  └─────────────────────────────────┘                      │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│  ステップ3: 対話結果                                           │
│  ┌──────────────────────────────────────┐                   │
│  │  • 文字起こし結果の表示                │                   │
│  │  • AI応答の表示                        │                   │
│  │  • 対話履歴の表示                      │                   │
│  └──────────────────────────────────────┘                   │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│  データ保存                                                    │
│                                                               │
│  ┌──────────────────────────────────────┐                   │
│  │  Supabase (PostgreSQL)               │                   │
│  │  • ユーザー名別の対話履歴             │                   │
│  │  • 感情座標、表情、文字起こし、AI応答  │                   │
│  └──────────────────────────────────────┘                   │
└─────────────────────────────────────────────────────────────┘
```

### ハイブリッド処理モデル

リソース制約を回避するため、重い処理を分散させる。

| 処理ブロック | 技術スタック | 実行場所 | 備考 |
|------------|------------|---------|------|
| **フロントエンド** | Streamlit | Streamlit (Local/Cloud) | UI構築および全体の制御 |
| **感情入力** | Plotly | ブラウザ | 2D感情プロットによる入力 |
| **映像入力** | streamlit-webrtc | ブラウザ ↔ Streamlit | リアルタイム映像通信 |
| **映像解析** | OpenCV, GPT-4o Vision | Streamlit + クラウド | 5秒間隔でフレーム抽出→GPT-4o Visionで分析 |
| **音声認識** | Whisper API | クラウド (OpenAI) | サーバー負荷軽減のためAPI利用 |
| **対話生成** | GPT-4o-mini | クラウド (OpenAI) | 高速レスポンス・低コスト |
| **データ保存** | Supabase (PostgreSQL) | クラウド | ユーザー名別の対話履歴管理 |

---

## 機能要件

### ユーザーインターフェース (UI)

#### ステップバイステップフロー

1. **ステップ1: 感情入力**
   - **2D感情プロット**: Plotlyによるインタラクティブな感情入力
   - **スライダー**: X軸（快/不快）、Y軸（覚醒/落ち着き）の調整
   - **クリック操作**: グラフ上をクリックして座標を設定

2. **ステップ2: 録画録音**
   - **WebRTC録画**: streamlit-webrtcによる録画開始/停止
   - **自動分析**: 録画停止後、自動的に文字起こしと表情認識を実行

3. **ステップ3: 対話結果**
   - **文字起こし結果**: Whisper APIによる文字起こし結果の表示
   - **AI応答**: GPT-4o-miniによる共感的な応答の表示
   - **対話履歴**: 過去の対話履歴の表示

#### ユーザー管理

- **ユーザー名入力**: 初回起動時にユーザー名を入力
- **ユーザー別履歴**: ユーザー名ごとに対話履歴を分離管理

### 表情認識ロジック

**実装ファイル**: `services/face_analysis.py`

**使用ライブラリ**: opencv-python-headless, GPT-4o Vision API

**処理フロー**:

1. WebM動画データからOpenCVでフレームを抽出（5秒間隔）
2. 各フレームをJPEG形式にエンコード
3. GPT-4o Vision APIに送信して表情を分析
4. 全フレームの分析結果を集計（最頻出の感情、平均信頼度）
5. 結果を対話生成時に参照できるようにする

**エラーハンドリング**: 解析エラー時（顔未検出など）は警告を表示するが処理は続行（`face_emotion_result = None`で続行）

### 音声・対話ロジック

**実装ファイル**: `services/transcription.py`, `services/ai_chat.py`

**音声認識**: WebM動画データから音声を抽出し、OpenAI Whisper APIへ送信

- **Whisper API**: エラー時は空文字列を返し、UI側でエラー表示
- **クリーンアップ**: 処理完了後、生成した一時ファイルを **必ず削除** (`os.remove`)

**プロンプト構築** (`services/ai_chat.py`):

- ユーザー発言（文字起こし結果）
- 感情座標（2Dプロットから入力）
- 表情分析結果（GPT-4o Vision、オプション）
- System Promptにより「共感的なカウンセラー」として振る舞わせる

### データベース (Supabase / PostgreSQL)

**実装ファイル**: `services/database.py`

Supabase (PostgreSQL) を使用。オプショナルで、データベースが利用できない場合はメモリのみモードで動作。

**テーブル構成 (`conversation_history`)**:

| カラム名 | 型 | 説明 |
|---------|---|------|
| `id` | SERIAL | PRIMARY KEY |
| `username` | TEXT | ユーザー名 |
| `timestamp` | TIMESTAMP | タイムスタンプ（デフォルト: CURRENT_TIMESTAMP） |
| `transcription` | TEXT | 文字起こし結果 |
| `emotion_x` | REAL | 感情座標X（快/不快） |
| `emotion_y` | REAL | 感情座標Y（覚醒/落ち着き） |
| `face_emotion` | TEXT | 表情分析結果（JSON形式） |
| `ai_response` | TEXT | AI応答 |

**インデックス**: `timestamp` に降順インデックスを作成し、検索を高速化

---

## 実装要件

### 表情認識モジュール (`services/face_analysis.py`)

- OpenCVでWebM動画からフレームを抽出（5秒間隔）
- GPT-4o Vision APIで各フレームの表情を分析
- 解析エラー時は警告を表示するが処理は続行（`face_emotion_result = None`で続行）

### 文字起こしモジュール (`services/transcription.py`)

- **Whisper API**: WebM動画データから音声を抽出して文字起こし
- エラー時は空文字列を返し、UI側でエラー表示
- **クリーンアップ**: 処理完了後、生成した一時ファイルを **必ず削除** (`os.remove`)

### AI対話モジュール (`services/ai_chat.py`)

- 文字起こし結果、感情座標、表情分析結果を統合してプロンプト構築
- GPT-4o-mini APIで共感的な応答を生成

### データベースモジュール (`services/database.py`)

- Supabase (PostgreSQL) を使用（オプショナル）
- データベースが利用できない場合はメモリのみモードで動作
- テーブル `conversation_history` に `timestamp` のインデックスを作成

### メインUI (`frontdesign.py`)

- **ステップバイステップフロー**: 感情入力 → 録画録音 → 対話結果の3ステップ
- **ローディング表示**: 音声処理やAPI通信中は、`st.status` を使用して進行状況を表示
- **エラー表示**: API制限やネットワークエラーが発生した場合、`st.error` で分かりやすくメッセージを表示
- **セッション管理**: `st.session_state` を活用し、ページリロードしても対話履歴が消えないようにする
- **ユーザー名管理**: 初回起動時にユーザー名を入力し、ユーザー別に履歴を管理

---

## プロジェクト構成

```
ABC_miniproject/
├── frontdesign.py           # メインUIアプリケーション
├── utils.py                 # 共通ユーティリティ（セッション管理、DB操作）
├── services/
│   ├── __init__.py
│   ├── ai_chat.py          # AI対話サービス
│   ├── face_analysis.py    # 表情認識サービス（GPT-4o Vision）
│   ├── transcription.py    # 文字起こしサービス（Whisper API）
│   ├── database.py         # データベース操作（Supabase）
│   └── INTERFACE.md        # サービスインターフェース仕様
├── requirements.txt        # Python依存パッケージ
├── ARCHITECTURE.md         # アーキテクチャドキュメント
├── README.md               # プロジェクト説明書
└── .streamlit/
    └── secrets.toml.example # 設定ファイルのテンプレート
```

---

## 技術スタック

- **Frontend**: Streamlit, streamlit-webrtc
- **可視化**: Plotly (2D感情プロット)
- **Vision**: opencv-python-headless (フレーム抽出), GPT-4o Vision API (表情認識)
- **Audio**: openai (Whisper API)
- **Database**: Supabase (PostgreSQL) - オプショナル、psycopg2-binary
- **AI**: openai (GPT-4o-mini, GPT-4o Vision)

---

## 開発チーム

### メンバー①：テックリード

**担当**: インフラ基盤 & 映像処理コア

**タスク**:

- Streamlit環境のセットアップ
- `services/face_analysis.py` の実装と `frontdesign.py` への統合
- GPT-4o Vision APIの最適化

### メンバー②：UI & データ担当（メンバーA）

**担当**: フロントエンド & データベース

**タスク**:

- StreamlitによるステップバイステップUIの実装
- Supabaseへの保存関数と読み込み関数の実装
- ユーザー名管理機能の実装

### メンバー③：AIロジック担当（メンバーB）

**担当**: API連携 & プロンプトエンジニアリング

**タスク**:

- Whisper API および ChatGPT API を叩く関数の実装
- AIの人格形成（System Promptの調整）
- 「感情座標、表情、文字起こし」をどうAIに伝えるかのロジック検討

---

## 開発ロードマップ

### Phase 1: Hello World (〜12/10)

- サーバー上でカメラが起動し、顔認識枠が表示されること
- 録音した音声がテキスト化されること

### Phase 2: モジュール結合 (〜12/20)

- 映像解析結果と音声テキストを合わせてGPTに送る
- 返答が画面に返ってくる

### Phase 3: UX改善 & 安定化 (〜12/31)

- DB保存の実装
- UIデザインの調整（使いやすさ向上）

### Phase 4: 最終調整 (1月)

- デモ用のシナリオ作成
- 予備機能（Google Cloud連携など）への挑戦（余裕があれば）

### オプション機能（時間があれば実装）

以下の機能は、MVP完了後に検討するオプション機能です。

- **トーン分析**: 音声のトーン（感情的なニュアンス）を詳細に分析
- **X-Y平面感情プロット**: 感情の推移を2次元グラフで可視化
- **Google Cloud連携**: より高度な分析機能の実装

---

## 注意事項

- 本システムはMVP版です
- OpenAI APIの利用にはAPIキーが必要です（有料）
  - GPT-4o-mini: 対話生成
  - GPT-4o Vision: 表情認識
  - Whisper API: 文字起こし
- カメラとマイクへのアクセス許可が必要です
- Supabase (PostgreSQL) の設定はオプショナルです。設定しない場合はメモリのみモードで動作します
- 詳細なアーキテクチャは [ARCHITECTURE.md](ARCHITECTURE.md) を参照してください

---

## ライセンス

[ライセンス情報を記載]
