"""ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ - ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§è‡ªå‹•é€²è¡Œ"""

import os
import tempfile
import streamlit as st
import plotly.graph_objects as go
from datetime import datetime
from aiortc.contrib.media import MediaRecorder
from streamlit_webrtc import WebRtcMode, webrtc_streamer
from utils import init_session_state, get_openai_client, save_conversation
from services.transcription import transcribe_video
from services.face_analysis import analyze_face_emotion
from services.ai_chat import generate_ai_response

st.set_page_config(
    page_title="AIå¯¾è©±æŒ¯ã‚Šè¿”ã‚Šãƒ¡ãƒ‡ã‚£ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ MVP",
    page_icon="ğŸ§˜",
    layout="wide",
    initial_sidebar_state="collapsed",  # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’æœ€åˆã‹ã‚‰é–‰ã˜ã‚‹
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
init_session_state()

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åã®ãƒã‚§ãƒƒã‚¯ã¨å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
if "username" not in st.session_state or not st.session_state["username"]:
    st.title("ğŸ§˜ AIå¯¾è©±æŒ¯ã‚Šè¿”ã‚Šãƒ¡ãƒ‡ã‚£ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆMVPï¼‰")
    st.markdown("---")
    st.subheader("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    st.markdown("å¯¾è©±å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    with st.form("username_form"):
        username_input = st.text_input(
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼å",
            placeholder="ä¾‹: å±±ç”°å¤ªéƒ",
            help="ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã§å¯¾è©±å±¥æ­´ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚"
        )
        submitted = st.form_submit_button("é–‹å§‹", type="primary")
        
        if submitted:
            if username_input and username_input.strip():
                username = username_input.strip()
                st.session_state["username"] = username
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼åè¨­å®šå¾Œã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
                from utils import load_conversation_history
                st.session_state["conversation_history"] = load_conversation_history(username)
                st.session_state["last_loaded_username"] = username
                st.success(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼åã€Œ{username}ã€ã§é–‹å§‹ã—ã¾ã™ã€‚")
                st.rerun()
            else:
                st.error("ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.stop()  # ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒè¨­å®šã•ã‚Œã‚‹ã¾ã§å‡¦ç†ã‚’åœæ­¢

# ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç®¡ç†ï¼ˆ1: æ„Ÿæƒ…å…¥åŠ›, 2: éŒ²ç”»éŒ²éŸ³, 3: å¯¾è©±çµæœï¼‰
if "current_step" not in st.session_state:
    st.session_state["current_step"] = 1

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—
client = get_openai_client()

st.title("ğŸ§˜ AIå¯¾è©±æŒ¯ã‚Šè¿”ã‚Šãƒ¡ãƒ‡ã‚£ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆMVPï¼‰")
# ãƒ¦ãƒ¼ã‚¶ãƒ¼åã®è¡¨ç¤º
st.caption(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {st.session_state['username']}")

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
steps = ["æ„Ÿæƒ…å…¥åŠ›", "éŒ²ç”»éŒ²éŸ³", "å¯¾è©±çµæœ"]
progress = (st.session_state["current_step"] - 1) / len(steps)
st.progress(
    progress,
    text=f"ã‚¹ãƒ†ãƒƒãƒ— {st.session_state['current_step']}/{len(steps)}: {steps[st.session_state['current_step'] - 1]}",
)

st.markdown("---")

# ============================
# ã‚¹ãƒ†ãƒƒãƒ—1: æ„Ÿæƒ…å…¥åŠ›
# ============================
if st.session_state["current_step"] == 1:
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—1: ğŸ­ æ„Ÿæƒ…å…¥åŠ›")
    st.markdown("ä»Šã®æ°—æŒã¡ã‚’2æ¬¡å…ƒã®æ„Ÿæƒ…ãƒ—ãƒ­ãƒƒãƒˆã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    left, right = st.columns([1, 1], gap="large")

    with left:
        st.write("**â‘  ä»Šã®æ„Ÿæƒ…ã‚’å…¥åŠ›ï¼ˆ2Dï¼‰**")

        x = st.slider(
            "Xè»¸ï¼šä¸å¿« â† 0 â†’ å¿«",
            min_value=-1.0,
            max_value=1.0,
            value=float(st.session_state["emotion_coords"][0]),
            step=0.01,
        )
        y = st.slider(
            "Yè»¸ï¼šéè¦šé†’ï¼ˆè½ã¡ç€ãï¼‰ â† 0 â†’ è¦šé†’",
            min_value=-1.0,
            max_value=1.0,
            value=float(st.session_state["emotion_coords"][1]),
            step=0.01,
        )

        if st.button(
            "ã“ã®åº§æ¨™ã§æ±ºå®š / æ¬¡ã¸é€²ã‚€", type="primary", use_container_width=True
        ):
            st.session_state["emotion_coords"] = (float(x), float(y))
            st.session_state["current_step"] = 2
            st.success(f"ä¿å­˜ã—ã¾ã—ãŸ: {st.session_state['emotion_coords']}")
            st.rerun()

    with right:
        st.write("**æ„Ÿæƒ…ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå¯è¦–åŒ–ãƒ»ã‚¯ãƒªãƒƒã‚¯ã§ç§»å‹•ï¼‰**")

        # Plotlyã‚°ãƒ©ãƒ•ä½œæˆ
        fig = go.Figure()

        # èƒŒæ™¯ã‚°ãƒªãƒƒãƒ‰
        for i in range(-1, 2):
            if i != 0:
                fig.add_hline(
                    y=i * 0.5,
                    line_width=0.5,
                    line_color="lightgray",
                    line_dash="dot",
                    opacity=0.3,
                )
                fig.add_vline(
                    x=i * 0.5,
                    line_width=0.5,
                    line_color="lightgray",
                    line_dash="dot",
                    opacity=0.3,
                )

        fig.add_hline(y=0, line_width=2, line_color="black", opacity=0.5)
        fig.add_vline(x=0, line_width=2, line_color="black", opacity=0.5)

        # ç¾åœ¨ã®ç‚¹
        fig.add_trace(
            go.Scatter(
                x=[x],
                y=[y],
                mode="markers",
                marker=dict(size=20, color="red", line=dict(width=3, color="darkred")),
                showlegend=False,
                hovertemplate="<b>ç¾åœ¨ã®åº§æ¨™</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>",
            )
        )

        # ä¿å­˜æ¸ˆã¿ã®ç‚¹
        saved_x, saved_y = st.session_state["emotion_coords"]
        if abs(saved_x - x) > 0.01 or abs(saved_y - y) > 0.01:
            fig.add_trace(
                go.Scatter(
                    x=[saved_x],
                    y=[saved_y],
                    mode="markers",
                    marker=dict(
                        size=15,
                        color="lightblue",
                        line=dict(width=2, color="blue"),
                        opacity=0.7,
                        symbol="circle-open",
                    ),
                    showlegend=False,
                    hovertemplate="<b>ä¿å­˜æ¸ˆã¿åº§æ¨™</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>",
                )
            )

        # ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªã‚°ãƒªãƒƒãƒ‰
        grid_step = 0.1
        grid_x_points = [
            i * grid_step
            for i in range(int(-1.0 / grid_step), int(1.0 / grid_step) + 1)
        ]
        grid_y_points = [
            i * grid_step
            for i in range(int(-1.0 / grid_step), int(1.0 / grid_step) + 1)
        ]
        grid_x_all = [gx for gx in grid_x_points for _ in grid_y_points]
        grid_y_all = [gy for gy in grid_y_points for _ in grid_x_points]

        fig.add_trace(
            go.Scatter(
                x=grid_x_all,
                y=grid_y_all,
                mode="markers",
                marker=dict(size=8, opacity=0.01, color="gray"),
                showlegend=False,
                hoverinfo="skip",
            )
        )

        fig.update_layout(
            xaxis=dict(
                range=[-1.1, 1.1], title="Pleasure (ä¸å¿« â† 0 â†’ å¿«)", zeroline=False
            ),
            yaxis=dict(
                range=[-1.1, 1.1],
                title="Arousal (éè¦šé†’ â† 0 â†’ è¦šé†’)",
                zeroline=False,
                scaleanchor="x",
                scaleratio=1,
            ),
            title="Current Emotion Point (ã‚°ãƒ©ãƒ•ä¸Šã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç§»å‹•)",
            width=500,
            height=500,
            dragmode="select",
            hovermode="closest",
            plot_bgcolor="white",
            paper_bgcolor="white",
        )

        selection = st.plotly_chart(
            fig, use_container_width=True, on_select="rerun", key="emotion_plot"
        )

        if selection and hasattr(selection, "selection") and selection.selection.points:
            try:
                for point_data in selection.selection.points:
                    if len(point_data) >= 2:
                        clicked_x = max(-1.0, min(1.0, float(point_data[0])))
                        clicked_y = max(-1.0, min(1.0, float(point_data[1])))
                        st.session_state["emotion_coords"] = (clicked_x, clicked_y)
                        st.success(
                            f"åº§æ¨™ã‚’æ›´æ–°ã—ã¾ã—ãŸ: ({clicked_x:.2f}, {clicked_y:.2f})"
                        )
                        st.rerun()
                        break
            except (AttributeError, IndexError, ValueError):
                pass

        st.info(
            f"ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ç¾åœ¨å€¤: (x, y)=({x:.2f}, {y:.2f}) / ä¿å­˜æ¸ˆã¿: {st.session_state['emotion_coords']}"
        )

# ============================
# ã‚¹ãƒ†ãƒƒãƒ—2: éŒ²ç”»éŒ²éŸ³
# ============================
elif st.session_state["current_step"] == 2:
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—2: ğŸ“¹ éŒ²ç”»ãƒ»éŒ²éŸ³")
    st.markdown("éŒ²ç”»ã‚’é–‹å§‹ã—ã¦ã€çµ‚äº†å¾Œã«è‡ªå‹•ã§åˆ†æã¸é€²ã¿ã¾ã™ã€‚")
    st.info(
        "ğŸ’­ **ä»Šæ—¥ä¸€æ—¥ã€ã©ã‚“ãªã“ã¨ãŒã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿæ¥½ã—ã‹ã£ãŸã“ã¨ã€å¤§å¤‰ã ã£ãŸã“ã¨ã€ä½•ã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚ã‚ãªãŸã®æ°—æŒã¡ã‚„è€ƒãˆã‚’ã€1åˆ†ã»ã©è‡ªç”±ã«è©±ã—ã¦ã¿ã¦ãã ã•ã„ã€‚**"
    )
    left2, right2 = st.columns([1, 1], gap="large")

    if (
        "recording_path" not in st.session_state
        or st.session_state["recording_path"] is None
    ):
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".webm")
        st.session_state["recording_path"] = temp_file.name
        temp_file.close()

    # ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£ã§recording_pathã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ï¼‰
    recording_path_value = st.session_state["recording_path"]

    def in_recorder_factory():
        return MediaRecorder(recording_path_value)

    with left2:
        st.write("**â‘¡ éŒ²ç”»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«**")
        try:
            ctx = webrtc_streamer(
                key="recorder",
                mode=WebRtcMode.SENDRECV,
                media_stream_constraints={"video": True, "audio": True},
                in_recorder_factory=in_recorder_factory,
                async_processing=True,
            )

            # ctxã¨ctx.stateãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
            if ctx is not None and ctx.state is not None:
                is_playing = ctx.state.playing
                
                if is_playing and not st.session_state["was_playing"]:
                    st.session_state["was_playing"] = True
                    st.session_state["recording_started_at"] = datetime.now().isoformat(
                        timespec="seconds"
                    )
                    st.session_state["recorded_video_data"] = None
                    st.session_state["transcription_result"] = None
                    st.session_state["transcription_status"] = "idle"
                    st.session_state["face_emotion_result"] = None
                    st.session_state["face_emotion_status"] = "idle"
                    st.session_state["ai_response"] = None
                    st.session_state["analysis_trigger"] = False

                if is_playing:
                    st.info("éŒ²ç”»ä¸­...")
                else:
                    if st.session_state["was_playing"]:
                        st.session_state["was_playing"] = False
                        recording_path = st.session_state.get("recording_path")
                        if recording_path and os.path.exists(recording_path):
                            file_size = os.path.getsize(recording_path)
                            st.write(f"éŒ²ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
                            if file_size < 100:
                                st.warning(
                                    "âš ï¸ éŒ²ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒå°ã•ã™ãã¾ã™ã€‚éŸ³å£°ãŒéŒ²éŸ³ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ã‚¯è¨±å¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                                )
                            with open(recording_path, "rb") as f:
                                recorded_bytes = f.read()
                            st.session_state["recorded_video_data"] = recorded_bytes
                            st.session_state["analysis_trigger"] = True
                            os.remove(recording_path)
                            st.session_state["recording_path"] = None
                            st.success("éŒ²ç”»ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚")
                            st.rerun()
                    st.info("åœæ­¢ä¸­")
            else:
                st.info("WebRTCæ¥ç¶šã‚’åˆæœŸåŒ–ä¸­...")
        except (AttributeError, RuntimeError) as e:
            # WebRTCæ¥ç¶šã®ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒã—ã¦å‡¦ç†ã‚’ç¶šè¡Œ
            st.warning(f"WebRTCæ¥ç¶šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            if st.session_state.get("was_playing"):
                # éŒ²ç”»ãŒåœæ­¢ã—ãŸå ´åˆã®å‡¦ç†
                st.session_state["was_playing"] = False
                recording_path = st.session_state.get("recording_path")
                if recording_path and os.path.exists(recording_path):
                    try:
                        file_size = os.path.getsize(recording_path)
                        st.write(f"éŒ²ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
                        if file_size < 100:
                            st.warning(
                                "âš ï¸ éŒ²ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒå°ã•ã™ãã¾ã™ã€‚éŸ³å£°ãŒéŒ²éŸ³ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒã‚¤ã‚¯è¨±å¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                            )
                        with open(recording_path, "rb") as f:
                            recorded_bytes = f.read()
                        st.session_state["recorded_video_data"] = recorded_bytes
                        st.session_state["analysis_trigger"] = True
                        os.remove(recording_path)
                        st.session_state["recording_path"] = None
                        st.success("éŒ²ç”»ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚")
                        st.rerun()
                    except Exception as cleanup_error:
                        st.error(f"éŒ²ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {cleanup_error}")
            st.info("åœæ­¢ä¸­")

    with right2:
        st.write("**â‘¢ çŠ¶æ…‹**")
        if st.session_state["recording_started_at"]:
            st.write(f"é–‹å§‹æ™‚åˆ»: {st.session_state['recording_started_at']}")
        st.info("éŒ²ç”»ã‚’æ­¢ã‚ã‚‹ã¨è‡ªå‹•ã§åˆ†æã«é€²ã¿ã¾ã™ã€‚")

        # æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¦ã„ã‚‹å ´åˆã€æ‰‹å‹•ã§æ¬¡ã¸é€²ã‚€ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
        if st.session_state.get("transcription_status") == "completed":
            st.markdown("---")
            if st.button(
                "âœ… æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ï¼ˆå¯¾è©±çµæœï¼‰",
                type="primary",
                use_container_width=True,
                key="next_to_step3",
            ):
                st.session_state["current_step"] = 3
                st.rerun()

    # éŒ²ç”»ãƒ‡ãƒ¼ã‚¿å—ä¿¡å¾Œã®è‡ªå‹•åˆ†æ
    if st.session_state.get("analysis_trigger"):
        st.session_state["analysis_trigger"] = False
        st.success("åéŒ²ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚æ–‡å­—èµ·ã“ã—ã¨è¡¨æƒ…èªè­˜å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")

        # éŒ²ç”»ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€æ–‡å­—èµ·ã“ã—ã¨è¡¨æƒ…èªè­˜å‡¦ç†ã‚’è‡ªå‹•å®Ÿè¡Œ
        if (
            st.session_state["recorded_video_data"] is not None
            and client is not None
            and "OPENAI_API_KEY" in st.secrets
        ):
            # æ–‡å­—èµ·ã“ã—å‡¦ç†
            with st.status(
                "éŒ²ç”»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºã—ã¦æ–‡å­—èµ·ã“ã—ä¸­...", expanded=True
            ) as status:
                try:
                    st.write("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                    st.write("Whisper APIã«é€ä¿¡ä¸­...ï¼ˆå‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºï¼‰")
                    st.session_state["transcription_status"] = "processing"

                    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã³å‡ºã—
                    transcription_text, transcription_status = transcribe_video(
                        st.session_state["recorded_video_data"], client
                    )

                    if transcription_status == "completed":
                        st.session_state["transcription_result"] = transcription_text
                        st.session_state["transcription_status"] = "completed"
                        status.update(
                            label="æ–‡å­—èµ·ã“ã—å®Œäº†ï¼",
                            state="complete",
                            expanded=False,
                        )
                    else:
                        st.session_state["transcription_status"] = "error"
                        st.error("æ–‡å­—èµ·ã“ã—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                        status.update(label="ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ", state="error")
                except Exception as e:
                    st.session_state["transcription_status"] = "error"
                    st.error(f"æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
                    status.update(label="ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ", state="error")

            # è¡¨æƒ…èªè­˜å‡¦ç†
            with st.status(
                "éŒ²ç”»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã¦è¡¨æƒ…èªè­˜ä¸­...", expanded=True
            ) as status_face:
                try:
                    st.write("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰5ç§’ã”ã¨ã«ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
                    st.write("GPT-4o Vision APIã«é€ä¿¡ä¸­...")
                    st.session_state["face_emotion_status"] = "processing"

                    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã³å‡ºã—
                    face_emotion, face_status = analyze_face_emotion(
                        st.session_state["recorded_video_data"], client
                    )

                    if face_status == "completed":
                        st.session_state["face_emotion_result"] = face_emotion
                        st.session_state["face_emotion_status"] = "completed"
                        status_face.update(
                            label="è¡¨æƒ…èªè­˜å®Œäº†ï¼",
                            state="complete",
                            expanded=False,
                        )
                    else:
                        st.session_state["face_emotion_status"] = "error"
                        st.warning("è¡¨æƒ…èªè­˜å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰")
                        st.session_state["face_emotion_result"] = None
                        status_face.update(
                            label="è¡¨æƒ…èªè­˜ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰",
                            state="error",
                            expanded=False,
                        )
                except Exception as e:
                    st.session_state["face_emotion_status"] = "error"
                    st.warning(f"è¡¨æƒ…èªè­˜ã‚¨ãƒ©ãƒ¼: {e}ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰")
                    st.session_state["face_emotion_result"] = None
                    status_face.update(
                        label="è¡¨æƒ…èªè­˜ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰",
                        state="error",
                        expanded=False,
                    )

            # æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ãŸã‚‰è‡ªå‹•çš„ã«æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ï¼ˆã“ã“ã§é·ç§»ï¼‰
            if st.session_state["transcription_status"] == "completed":
                st.session_state["current_step"] = 3
                st.rerun()
        else:
            st.warning(
                "éŒ²ç”»ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã¾ãŸã¯APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            )

# ============================
# ã‚¹ãƒ†ãƒƒãƒ—3: å¯¾è©±çµæœ
# ============================
elif st.session_state["current_step"] == 3:
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—3: ğŸ’¬ å¯¾è©±ãƒ»çµæœ")
    st.markdown("æ–‡å­—èµ·ã“ã—çµæœã¨AIå¿œç­”ã‚’ç¢ºèªã§ãã¾ã™ã€‚")

    # æ–‡å­—èµ·ã“ã—çµæœã®è¡¨ç¤º
    if st.session_state["transcription_result"]:
        st.markdown("---")
        st.subheader("ğŸ“ æ–‡å­—èµ·ã“ã—çµæœ")
        if st.session_state["transcription_status"] == "completed":
            st.success(st.session_state["transcription_result"])

            # è‡ªå‹•çš„ã«AIå¿œç­”ã‚’ç”Ÿæˆï¼ˆã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
            if (
                client is not None
                and "OPENAI_API_KEY" in st.secrets
                and st.session_state["ai_response"] is None
            ):
                with st.spinner("AIå¿œç­”ã‚’è‡ªå‹•ç”Ÿæˆä¸­..."):
                    try:
                        # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã³å‡ºã—
                        ai_response, response_status = generate_ai_response(
                            st.session_state["transcription_result"],
                            st.session_state["emotion_coords"],
                            face_emotion=st.session_state.get("face_emotion_result"),
                            client=client,
                        )

                        if response_status == "completed":
                            st.session_state["ai_response"] = ai_response

                            # å¯¾è©±å±¥æ­´ã«è¿½åŠ ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚‚ä¿å­˜ï¼‰
                            conversation_data = {
                                "transcription": st.session_state[
                                    "transcription_result"
                                ],
                                "emotion": st.session_state["emotion_coords"],
                                "face_emotion": st.session_state.get(
                                    "face_emotion_result"
                                ),
                                "ai_response": st.session_state["ai_response"],
                                "timestamp": datetime.now().isoformat(),
                            }
                            save_conversation(conversation_data, st.session_state.get("username"))

                            st.rerun()
                        else:
                            st.error("AIå¿œç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                    except Exception as e:
                        st.error(f"AIå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            elif (
                client is not None
                and "OPENAI_API_KEY" in st.secrets
                and st.session_state["ai_response"] is not None
            ):
                if st.button("ğŸ”„ AIå¿œç­”ã‚’å†ç”Ÿæˆ", type="secondary"):
                    st.session_state["ai_response"] = None
                    st.rerun()
        elif st.session_state["transcription_status"] == "error":
            st.error("æ–‡å­—èµ·ã“ã—å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

    # AIå¿œç­”ã®è¡¨ç¤º
    if st.session_state["ai_response"]:
        st.markdown("---")
        st.subheader("ğŸ’¬ AIå¿œç­”")
        st.info(st.session_state["ai_response"])

    # å¯¾è©±å±¥æ­´ã®è¡¨ç¤º
    if st.session_state["conversation_history"]:
        st.markdown("---")
        st.subheader("ğŸ“š å¯¾è©±å±¥æ­´")
        for i, conv in enumerate(reversed(st.session_state["conversation_history"])):
            with st.expander(
                f"å¯¾è©± {len(st.session_state['conversation_history']) - i} - {conv.get('timestamp', '')[:10]}"
            ):
                st.write(f"**æ„Ÿæƒ…åº§æ¨™:** {conv['emotion']}")
                if conv.get("face_emotion"):
                    face_info = conv["face_emotion"]
                    dominant = face_info.get("dominant_emotion", "unknown")
                    confidence = face_info.get("confidence", 0.0)
                    frame_count = face_info.get("frame_count", 0)
                    st.write(
                        f"**è¡¨æƒ…åˆ†æ:** {dominant} (ä¿¡é ¼åº¦: {confidence:.2f}, åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_count})"
                    )
                st.write(f"**ã‚ãªãŸ:** {conv['transcription']}")
                st.write(f"**AI:** {conv['ai_response']}")

    # æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™ãƒœã‚¿ãƒ³
    st.markdown("---")
    if st.button("ğŸ”„ æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™", type="primary", use_container_width=True):
        st.session_state["current_step"] = 1
        st.session_state["is_recording"] = False
        st.session_state["transcription_result"] = None
        st.session_state["transcription_status"] = "idle"
        st.session_state["face_emotion_result"] = None
        st.session_state["face_emotion_status"] = "idle"
        st.session_state["ai_response"] = None
        st.rerun()
